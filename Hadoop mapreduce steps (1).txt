Hadoop mapreduce steps : 

Files : UserLogs.java
	

package Logs;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;

public class UserLogs {
	public static void main(String[] args) {
		JobClient my_client = new JobClient();
		// Create a configuration object for the job
		JobConf job_conf = new JobConf(userLogs.class);

		// Set a name of the Job
		job_conf.setJobName("UserLog");

		// Specify data type of output key and value
		job_conf.setOutputKeyClass(Text.class);
		job_conf.setOutputValueClass(IntWritable.class);

		// Specify names of Mapper and Reducer Class
		job_conf.setMapperClass(Logs.LogsMapper.class);
		job_conf.setReducerClass(Logs.LogsReducer.class);

		// Specify formats of the data type of Input and output
		job_conf.setInputFormat(TextInputFormat.class);
		job_conf.setOutputFormat(TextOutputFormat.class);

		// Set input and output directories using command line arguments, 
		//arg[0] = name of input directory on HDFS, and arg[1] =  name of output directory to be created to store the output file.
		
		FileInputFormat.setInputPaths(job_conf, new Path(args[0]));
		FileOutputFormat.setOutputPath(job_conf, new Path(args[1]));

		my_client.setConf(job_conf);
		try {
			// Run the job 
			JobClient.runJob(job_conf);
		} catch (Exception e) {
			e.printStackTrace();
		}
	}
}

---------------------------------------------
    : LogsReducer.java
package Logs;

import java.io.IOException;
import java.util.*;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.*;

public class LogsReducer extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {

	public void reduce(Text t_key, Iterator<IntWritable> values, OutputCollector<Text,IntWritable> output, Reporter reporter) throws IOException {
		Text key = t_key;
		int frequencyOfLogs = 0;
		while (values.hasNext()) {
			// replace type of value with the actual type of our value
			IntWritable value = (IntWritable) values.next();
			frequencyOfLogs += value.get();
			
		}
		output.collect(key, new IntWritable(frequencyOfLogs));
	}
}

-----------------------------------------------
	: LogsMapper.java

package Logs;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.*;

public class LogsMapper extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {
	private final static IntWritable one = new IntWritable(1);

	public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {

		String valueString = value.toString();
		String[] SingleUserData = valueString.split("-");
		output.collect(new Text(SingleUserData[0]), one);
	}
}

-----------------------------------------------


Commands: 

sudo chown -R hdp123 analyselogs/
sudo chmod +r *.*


1. export CLASSPATH="$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.10.2.jar:$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.10.2.jar:$HADOOP_HOME/share/hadoop/common/hadoop-common-2.10.2.jar:~/analyselogs/Logs/*:$HADOOP_HOME/lib/*"

2. javac -d . LogsMapper.java LogsReducer.java UserLogs.java

3. sudo gedit Manifest.txt
  --> Main-Class: Logs.userLogs

4. jar -cfm analyselogs.jar Manifest.txt Logs/*.class

5. start-all.sh

6. sudo mkdir ~/input62

7. sudo cp log.csv ~/input62

8. $HADOOP_HOME/bin/hdfs dfs -put ~/input62 /

9. $HADOOP_HOME/bin/hadoop jar analyselogs.jar  /input62 /output62

10. $HADOOP_HOME/bin/hdfs dfs -cat /output62/part-00000